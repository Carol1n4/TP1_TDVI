---
title: "Tp1-TD6-Fuchs-Olivera-Glusman"
author: "Carolina Olivera, Milena Fuchs, Agustina Glusman"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
#install.packages("recipes") #Descomentar si no lo tienen ya instalado.
library(rpart)
library(rpart.plot)
library(ggplot2)
library(dplyr)
library(janitor)
library(caret)
library(tidyr)
library(pROC)
```

# Ejercicio 1

## **Introducción al problema seleccionado**

### **Problema a resolver**

El conjunto de datos seleccionado es `loan_data.csv`, que contiene
45.000 registros y 14 variables relacionadas con solicitantes de
préstamos. La información tiene características demográficas,
financieras y crediticias de los individuos junto con detalles
específicos de los préstamos. La variable objetivo es `loan_status`, que
indica si un crédito fue aprobado (1) o rechazado (0), generando un
problema de **clasificación binaria**, es decir, predecir si un préstamo
se aprueba o se rechaza según el perfil de la persona y del crédito.

Dentro de las variables principales se incluyen:

-   **Demográficas:** `person_age`, `person_gender`, `person_education`,
    `person_emp_exp`, `person_home_ownership`.
-   **Financieras:** `person_income`, `credit_score`,
    `cb_person_cred_hist_length`, `previous_loan_defaults_on_file`.
-   **Del préstamo:** `loan_amnt`,
    `loan_int_rate`,`loan_percent_income`, `loan_intent`.

En este trabajo utilizamos árboles de decisión como modelo principal, ya
que resultan apropiados para problemas de clasificación y permiten
reconocer de forma clara qué factores están más asociados a la
aprobación o rechazo de un préstamo. Además, estos pueden trabajar con
distintos tipos de variables, tanto numéricas como categóricas, lo que
los hace una herramienta flexible y adecuada para este tipo de análisis.

**Origen del dataset:**
<https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data>

# Ejercicio 2

## **Preparación de los datos**

En esta sección cargamos el dataset `loan_data.csv`, realizamos un
preprocesamiento mínimo y un análisis exploratorio que incluye
estadísticas descriptivas, visualizaciones y comentarios sobre las
principales características observadas.

```{r setup, include=FALSE}
datos <- read.csv('loan_data.csv', stringsAsFactors = FALSE)


# Preprocesamiento
datos$loan_status <- factor(datos$loan_status,
                           levels = c("0","1"),
                           labels = c("Rechazado","Aprobado"))

datos$person_gender <- as.factor(datos$person_gender)
datos$person_education <- as.factor(datos$person_education)
datos$person_home_ownership <- as.factor(datos$person_home_ownership)
datos$loan_intent <- as.factor(datos$loan_intent)
datos$previous_loan_defaults_on_file <- as.factor(datos$previous_loan_defaults_on_file)

# Chequeo de faltantes
colSums(is.na(datos))

```

El chequeo de valores faltantes mostró que ninguna variable presenta
datos nulos, por lo que se puede trabajar directamente con el dataset
completo.

```{r}

# 2.3 Estadísticas descriptivas
# Numéricas principales
summary(datos[, c("person_age","person_income","loan_amnt","loan_int_rate","credit_score")])

# Categóricas principales
table(datos$loan_status)
prop.table(table(datos$loan_status))
table(datos$loan_intent)

```

A partir de lo anterior podemos observar que la edad promedio de los
solicitantes es de 28 años, concentrada entre 24 y 30, aunque aparecen
valores extremos poco realistas como 144. Los ingresos muestran gran
dispersión ya que la media ronda los 80.000, pero con un rango que va de
8.000 a más de 7 millones. Los préstamos solicitados se concentran
alrededor de 9.500, con tasas de interés que oscilan entre 5,4% y 20%
(media cercana al 11%). El `credit_score` varía entre 390 y 850, con
promedio en 633.

La variable objetivo presenta un claro desbalance: 78% de préstamos
rechazados frente a 22% aprobados. En cuanto a la intención del
préstamo, destacan educación y gastos médicos como los motivos más
frecuentes, mientras que mejoras en el hogar aparece como el menos
común.

## Visualizaciones exploratorias

Para complementar el análisis, se generaron visualizaciones que permiten
observar de manera más clara el comportamiento de las variables y su
relación con el estado del préstamo.

```{r}
# Gráfico 1: Distribución de loan_status
ggplot(datos, aes(x = loan_status)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribución de estados del préstamo",
       x = "Estado", y = "Cantidad") +
  theme_minimal(base_size = 12)

```

El gráfico de barras muestra que la mayoría de los préstamos fueron
rechazados (78%), mientras que solo el 22% fueron aprobados. Para
nuestro análisis, este desbalance implica que no alcanza con medir la
exactitud de un modelo, ya que podría sesgarse hacia la clase
mayoritaria; por eso es necesario tener en cuenta otras métricas que
reflejen mejor la capacidad de identificar los casos aprobados como la
precisión, el recall o el F1-score.

```{r}
# Gráfico 2: Ingresos por loan_status

ggplot(datos, aes(x = loan_status, y = person_income)) +
  geom_boxplot(fill = "lightgreen", outlier.alpha = 0.3) +
  scale_y_continuous(labels = scales::label_number(big.mark = ".", decimal.mark = ",")) +
  labs(title = "Ingresos por estado del préstamo",
       x = "Estado del préstamo", y = "Ingreso anual") +
  theme_minimal(base_size = 12)

```

El boxplot permite comparar los ingresos de los solicitantes en función
del estado del préstamo. Se observa que, en promedio, quienes obtuvieron
aprobación tienden a tener ingresos más altos que los rechazados,aunque
la diferencia no es tan marcada. Además, aparecen valores extremos muy
elevados (outliers) que generan gran dispersión en ambos grupos.

## Ejercicio 3 **Construcción de un árbol de decisión básico**

Dividimos el conjunto de datos al azar en tres particiones:
entrenamiento (70%), validación (15%) y testeo (15%). Utilizamos una
semilla para asegurar la replicabilidad.

```{r}


#Guardamos la cantidad de muestras en datos_totales:
datos_totales<- nrow(datos)

#calulamos la cantidad de muestras para cada conjunto de datos (test, train y validation):
tamano_entrenamiento <- 0.7 * datos_totales
tamano_validacion <- 0.15 * datos_totales
tamano_testeo <- datos_totales - tamano_entrenamiento - tamano_validacion

set.seed(1234)
indices <- sample(1:datos_totales)

entrenamiento_indices <- indices[1:tamano_entrenamiento]  # 70% de los datos
validacion_indices <- indices[(tamano_entrenamiento + 1):(tamano_entrenamiento + tamano_validacion)]  # 15% de los datos
testeo_indices <- indices[(tamano_entrenamiento + tamano_validacion + 1):datos_totales]  # 15% restantes

# Crear los tres conjuntos de datos
train_data <- datos[entrenamiento_indices, ]
valid_data <- datos[validacion_indices, ]
test_data <- datos[testeo_indices, ]
```

```{r}

tree <- rpart(formula = loan_status ~ person_age + person_gender + person_education + person_income + person_emp_exp + person_home_ownership + loan_amnt + loan_intent + loan_int_rate + loan_percent_income+ cb_person_cred_hist_length+ credit_score+ previous_loan_defaults_on_file, 
              data = train_data, 
              method = "class")
rpart.control()

```

El modelo de árbol se construyó con los valores por defecto de rpart.
Estos hiperparámetros controlan el crecimiento del árbol. Por ejemplo,
minsplit = 20 asegura que un nodo no se divida si tiene menos de 20
observaciones, mientras que cp = 0.01 evita que el modelo se vuelva
excesivamente complejo al requerir que cada nueva división mejore al
menos un 1% el ajuste. El valor de maxdepth = 30 es suficientemente
grande como para no limitar el crecimiento en este caso.

Visualizamos el árbol:

```{r}


rpart.plot(tree)

 
```

### Interpretación del árbol de decisión

El árbol se construyó con `rpart()`, especificando como variable
respuesta `loan_status` (Aprobado/Rechazado) y como predictoras
distintas características del solicitante y del préstamo (edad,ingresos,
historial crediticio, monto, tasa, etc.).

-   Los **colores** de los nodos indican la clase predicha (verde =
    *Aprobado*, azul = *Rechazado*).

-   La **intensidad** del color refleja la **pureza** del nodo (qué tan
    homogéneo es).

-   Dentro de cada nodo se ve la **proporción** de la clase mayoritaria
    y el **% de observaciones** que llegan a ese nodo.

-   Las **reglas** aparecen en las ramas y marcan la condición que manda
    cada observación a la izquierda o a la derecha.

**Primeros cortes:**

-   **Primer corte -- `previous_loan_defaults_on_file`:** es la variable
    más determinante. Si el solicitante tiene incumplimientos previos,
    el modelo lo clasifica directamente como **Rechazado**.

-   **Segundo corte --`loan_percent_income` (\< 0.25):** Si la cuota
    representa **menos del 25%** del ingreso, la **probabilidad de
    aprobación** sube fuerte.

-   **Cortes posteriores -- `loan_int_rate`, `person_income`,
    `person_home_ownership`:** en general, **tasas más bajas**,
    **ingresos más altos** y **ser propietario** (u hipoteca) empujan
    hacia **Aprobado**.

En conjunto, el árbol prioriza señales de **riesgo histórico** (defaults
previos) y de **capacidad de pago** (porcentaje del ingreso, tasa e
ingresos).

# Ejercicio 4

## \*Metricas de performance\*\*

```{r}

# Predicción de clases
pred_clases <- predict(tree, newdata = test_data, type = "class")

# Predicción de probabilidades
pred_probabilidades <- predict(tree, newdata = test_data, type = "prob")

resultados <- data.frame(
  Clase_Predicha = pred_clases,
  Probabilidades = pred_probabilidades
)

head(resultados, 10)

```

Realizamos predicciones sobre el conjunto de testeo. Por un lado,
obtuvimos las **clases predichas** según la regla por defecto de
`rpart`. Por otro lado, calculamos las **probabilidades de pertenecer a
cada clase**. En la tabla inicial se ven casos con probabilidades
cercanas a 1 y otros más repartidos, lo que refleja distintos niveles de
confianza del modelo.

Importamos las librerías necesarias

```{r}
library(MLmetrics)
library(pROC)
```

Convertimos las predicciones y la variable objetivo en factores y
guardamos la predicción del conjunto de testeo en una variable:

```{r}
test_data$loan_status <- as.factor(test_data$loan_status)
pred <- predict(tree, newdata = test_data, type = "class")
```

### Matriz de confusión:

```{r}
conf_matrix <- confusionMatrix(pred, test_data$loan_status)
conf_matrix$table

# Convertir a data.frame para graficar
df_matrix <- as.data.frame(conf_matrix$table)

# Gráfico
ggplot(df_matrix, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(title = "Matriz de Confusión", x = "Predicción", y = "Real") +
  theme_minimal()
```

El modelo es muy bueno detectando Rechazados (5026 vs 185) por lo que
casi siempre que dice “Rechazado”, acierta. En cambio, es débil para
detectar Aprobados (426 correctos vs 1113 errados), lo que era esperable
dado el desbalance en los datos.

Esto significa que el árbol favorece la clase que tiene mas cantidad de
datos que en este caso es Rechazado.

### Accuracy:

```{r}
accuracy <- Accuracy(pred, test_data$loan_status)
accuracy
```

El modelo acierta en alrededor del 91% de los casos del conjunto de
test.

Como vimos en la matriz de confusión, el rendimiento para identificar la
clase Aprobado es mas bajo. Por lo que concluimos en que el valor del
accuracy está condicionado. La mayoría de las observaciones son
Rechazado, y el modelo tiende a predecir esa clase.

### Precision y Recall:

```{r}
precision_val <- Precision(test_data$loan_status,pred,positive = NULL)
recall_val <-Recall(test_data$loan_status,pred,positive = NULL)

precision_val
recall_val
```

Precision es aproximadamente 0.92. Significa que de todos los casos que
el modelo predijo como positivos, alrededor del 92% realmente lo eran.

Recall es aproximadamente 0.96. Significa que de los que realmente son
positivos, el modelo detecta el 96,4%.

El modelo tomó como positivos a los Rechazados. Es por esto que ambos
valores son muy altos.

### F1-score

```{r}
f_1_score<- F1_Score(test_data$loan_status,pred,positive = NULL)
f_1_score
```

El F1-Score es la media armónica entre precision y recall. Esto indica
que no solo predice la mayoría de los rechazados reales, sino que
tambien mantiene bajo el número de falsorechazados.

El modelo alcanza un **F1-Score de 0.94** para la clase mayoritaria
(*Rechazado*), lo que muestra un buen balance entre precisión y recall.
Este valor confirma que predice bien los rechazados, aunque no
representa el desempeño sobre la clase minoritaria (*Aprobado*), donde
el rendimiento es más bajo.

### AUC-ROC

```{r}
probabilities <- predict(tree, newdata = test_data, type = "prob")

auc_value <- MLmetrics::AUC(y_pred = probabilities[, "Aprobado"], y_true = as.numeric(test_data$loan_status == "Aprobado"))
print(paste("AUC:", auc_value))
```

El modelo alcanza un **AUC de 0.94**, esto signific que diferencia bien
entre *Aprobado* y *Rechazado*. Aunque el valor es muy bueno, el
desbalance de clases limita su efectividad en la predicción de
*Aprobado*.

## Optimización del modelo

Usamos **random search** para realizar una búsqueda aleatoria de
hiperparámetros, con `cp = 0` y `xval = 0`, para que el árbol crezca
hasta el límite fijado por los valores elegidos de maxdepth, minsplit y
minbucket. Probamos **1000** combinaciones con: `maxdepth ∈ [2, 30]`,
`minsplit ∈ [10, 500]` y `minbucket = round(minsplit/3)`. Evaluamos solo
con **AUC-ROC** en validación (clase positiva: *Aprobado*).

El **mejor modelo** alcanzó **AUC-ROC = 0.9672** con `maxdepth = 19`,
`minsplit = 34` y `minbucket = 11`. Con estos valores el árbol llega a
ser bastante profundo (19 niveles), pero como le ponemos límites al
tamaño de los nodos y de las hojas, no se descontrola. Eso le permite
separar bien las clases sin caer en overfitting.

```{r}

random_search <- function(train_data, valid_data, n) {
  # asegurar niveles (negativo, positivo)
  lvl <- c("Rechazado","Aprobado")
  train_data$loan_status <- factor(train_data$loan_status, levels = lvl)
  valid_data$loan_status   <- factor(valid_data$loan_status, levels = lvl)
  
  # resultados
  results <- data.frame(
    maxdepth = integer(),
    minsplit = integer(),
    minbucket = integer(),
    AUC_Validation = numeric(),
    stringsAsFactors = FALSE
  )
  
  set.seed(1234)
  
  for (i in 1:n) {
    # hiperparámetros aleatorios
    maxdepth  <- sample(2:30, 1)
    minsplit  <- sample(10:500, 1)
    minbucket <- max(1L, round(minsplit / 3)) 
    
    
    # entrenar con cp=0 y xval=0 
    tree_model <- suppressMessages(suppressWarnings(
      rpart(loan_status ~ .,
            data = train_data,
            method = "class",
            control = rpart.control(minsplit = minsplit,
                                    minbucket = minbucket,
                                    cp = 0, xval = 0,
                                    maxdepth = maxdepth))
    ))
    
    # probas en validación (tomamos la columna "Aprobado" por NOMBRE)
    probability_pred <- predict(tree_model, newdata = valid_data, type = "prob")
    prob_pos <- probability_pred[, "Aprobado"]
    
    # AUC en validación (con levels explícitos)
    roc_curve <- roc(response = valid_data$loan_status, predictor = prob_pos,
                     levels = lvl, quiet = TRUE)
    auc_value <- as.numeric(auc(roc_curve))
    
    # guardar
    results <- rbind(results, data.frame(maxdepth, minsplit, minbucket,
                                         AUC_Validation = auc_value))
  }
  
  # mejor por AUC
  best_model <- results[which.max(results$AUC_Validation), , drop = FALSE]
  return(list(best_model = best_model, results = results))
}

```

```{r}

rs <- random_search(train_data, valid_data, n = 1000)  
rs$best_model    # hiperparámetros y AUC de validación del mejor
```

### Visualización de la relación entre hiperparámetros y AUC-ROC

Los gráficos muestran cómo cambian los valores de AUC-ROC según las
distintas combinaciones de hiperparámetros probadas.

```{r}

plot_data <- rs$results %>%
  pivot_longer(cols = c("maxdepth", "minsplit", "minbucket"),
               names_to = "Hiperparámetro", values_to = "Valor")

ggplot(plot_data, aes(x = Valor, y = AUC_Validation)) +
  geom_point(alpha = 0.5, color = "steelblue") +
  geom_smooth(method = "loess", se = FALSE, color = "darkred", size = 1) +
  facet_wrap(~ Hiperparámetro, scales = "free_x") +
  labs(title = "Impacto de los hiperparámetros en el AUC-ROC (validación)",
       x = "Valor del hiperparámetro",
       y = "AUC-ROC") +
  theme_minimal(base_size = 13)
```

Se observa que el AUC-ROC mejora rápido con mayor profundidad hasta
estabilizarse, mientras que valores muy altos de *minsplit* y
*minbucket* tienden a reducir el desempeño.

### Elección del mejor árbol y evaluación en test

Tomamos la configuración con **mejor AUC-ROC en validación** y
reentrenamos el árbol sobre **train + valid** (manteniendo `cp = 0` y
`xval = 0`, y los niveles de la respuesta). Luego predecimos
**probabilidades** en **test** y medimos el **AUC-ROC** para estimar el
desempeño final.

```{r}
best_tree <- rpart(
  loan_status ~ ., 
  data = rbind(train_data,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs$best_model$minsplit,
    minbucket = rs$best_model$minbucket,
    maxdepth  = rs$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

prob_best <- predict(best_tree, newdata = test_data, type = "prob")[, "Aprobado"]

```

```{r}

lvl <- c("Rechazado","Aprobado")
train_data$loan_status <- factor(train_data$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree <- rpart(
  loan_status ~ ., 
  data = rbind(train_data,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs$best_model$minsplit,
    minbucket = rs$best_model$minbucket,
    maxdepth  = rs$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test <- predict(best_tree, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test,
                levels = lvl,
                quiet = TRUE)

auc_test <- auc(roc_test)
auc_test

```

### Comparación con el árbol básico

En el árbol básico del punto 3, el AUC-ROC en test fue de **0.94**,
mientras que el árbol optimizado alcanzó **0.9675**. La diferencia
muestra que ajustar hiperparámetros como **minsplit** y **maxdepth**
permitió que el modelo separe mejor las clases. El árbol optimizado
logra aprovechar mayor profundidad y divisiones mínimas más estrictas
para mejorar la calidad de las predicciones, manteniendo un buen
rendimiento en test sin caer en **overfitting**.

# Ejercicio 5

```{r}
rpart.plot(best_tree)
```

**Interpretación de resultados**

El árbol optimizado es mucho más grande, profundo y detallado que el
básico del punto 3, gracias a los hiperparámetros ajustados
(`maxdepth = 19`, `minsplit = 34`, `minbucket = 11`). Esto hace que
tenga más ramas y pueda detectar relaciones más específicas, a
diferencia del modelo inicial que quedaba más corto y con menos
divisiones. Así logra mejorar la performance, aunque con el costo de ser
más complejo de interpretar.

Otra diferencia importante es que el árbol básico se apoyaba casi
exclusivamente en un par de variables clave, mientras que el optimizado
incorpora un mayor número de features en los cortes. Esto muestra cómo
el ajuste de hiperparámetros le da más flexibilidad para aprovechar
información adicional y así mejorar la clasificación.

Por otro lado, el modelo del Ejercicio 3, aunque más simple y fácil de
interpretar, tiene una menor capacidad predictiva. Esto podría hacerlo
más útil en escenarios donde se valore la claridad del modelo, pero a
costa de un menor rendimiento en la clasificación.

```{r}
# Importancia de variables en el árbol optimizado
importance <- best_tree$variable.importance

# Ordenar de mayor a menor
importance <- sort(importance, decreasing = TRUE)
importance

importance_df <- data.frame(
  Variable = names(importance),
  Importancia = round(importance, 2)
)
importance_df
```

Como el árbol optimizado es bastante grande y difícil de leer en
detalle, usamos la función de importancia de variables
(`best_tree$variable.importance`) para identificar cuáles fueron las más
influyentes en las decisiones. De esta forma confirmamos que
**previous_loan_defaults_on_file** y **loan_percent_income** siguen
siendo las más determinantes, seguidas por variables como
**credit_score**, **loan_int_rate**, **person_income** y
**person_home_ownership**, que ayudan a refinar la clasificación en
ramas más profundas.

Este resultado tiene lógica desde el punto de vista crediticio ya que
los incumplimientos previos y la carga de la cuota sobre el ingreso son
factores importantes para decidir, mientras que las condiciones
financieras (tasa, score, monto) ayudan a mejorar la clasificación.

# Ejercicio 6

## **Análisis del impacto de etiquetas corruptas**

```{r}
corrupt_labels <- function(data, p, seed = 1234) {
  set.seed(seed)
  datos_corruptos <- data
  
  n <- nrow(data)
  n_corrupt <- round(p * n)
  indices_corrupt <- sample(1:n, n_corrupt, replace = FALSE)
  
  # invertir etiquetas
  for (i in indices_corrupt) {
    if (datos_corruptos$loan_status[i] == "Aprobado") {
      datos_corruptos$loan_status[i] <- "Rechazado"
    } else {
      datos_corruptos$loan_status[i] <- "Aprobado"
    }
  }
  return(datos_corruptos)
}

```

Armamos los nuevos datos de train corruptos:

```{r}
corruptos1 <- corrupt_labels(data = train_data, p = 0.05)
corruptos2 <- corrupt_labels(data = train_data, p = 0.10)
corruptos3 <- corrupt_labels(data = train_data, p = 0.15)
corruptos4 <- corrupt_labels(data = train_data, p = 0.20)
corruptos5 <- corrupt_labels(data = train_data, p = 0.25)
corruptos6 <- corrupt_labels(data = train_data, p = 0.30)
```

Buscamos el mejor arbol con los datos de validación:

```{r}
rs1 <- random_search(corruptos1, valid_data, n = 1000)  


rs2 <- random_search(corruptos2, valid_data, n = 1000)  


rs3 <- random_search(corruptos3, valid_data, n = 1000)  


rs4 <- random_search(corruptos4, valid_data, n = 1000)  


rs5 <- random_search(corruptos5, valid_data, n = 1000)  


rs6 <- random_search(corruptos6, valid_data, n = 1000)  

```

Creamos una tabla para poder ver los valores y compararlos

```{r}

rs_map <- list(
  "5%"  = rs1,
  "10%" = rs2,
  "15%" = rs3,
  "20%" = rs4,
  "25%" = rs5,
  "30%" = rs6
)

tab <- do.call(rbind, lapply(names(rs_map), function(pct){
  bm <- rs_map[[pct]]$best_model
  data.frame(
    id             = pct,
    maxdepth       = bm$maxdepth,
    minsplit       = bm$minsplit,
    minbucket      = bm$minbucket,
    AUC_Validation = round(bm$AUC_Validation, 4),
    row.names = NULL
  )
}))

knitr::kable(tab, align = "c", caption = "Random search por % de etiquetas corruptas")


```

### Corruptos 5%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos1$loan_status <- factor(corruptos1$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs1 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos1,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs1$best_model$minsplit,
    minbucket = rs1$best_model$minbucket,
    maxdepth  = rs1$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs1 <- predict(best_tree_rs1, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs1,
                levels = lvl,
                quiet = TRUE)

auc_test_rs1 <- auc(roc_test)
auc_test_rs1

```

### Corruptos 10%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos2$loan_status <- factor(corruptos2$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs2 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos2,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs2$best_model$minsplit,
    minbucket = rs2$best_model$minbucket,
    maxdepth  = rs2$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs2 <- predict(best_tree_rs2, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs2,
                levels = lvl,
                quiet = TRUE)

auc_test_rs2 <- auc(roc_test)
auc_test_rs2

```

### Corruptos 15%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos3$loan_status <- factor(corruptos3$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs3 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos3,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs3$best_model$minsplit,
    minbucket = rs3$best_model$minbucket,
    maxdepth  = rs3$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs3 <- predict(best_tree_rs3, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs3,
                levels = lvl,
                quiet = TRUE)

auc_test_rs3 <- auc(roc_test)
auc_test_rs3

```

### Corruptos 20%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos4$loan_status <- factor(corruptos4$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs4 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos4,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs4$best_model$minsplit,
    minbucket = rs4$best_model$minbucket,
    maxdepth  = rs4$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs4 <- predict(best_tree_rs4, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs4,
                levels = lvl,
                quiet = TRUE)

auc_test_rs4 <- auc(roc_test)
auc_test_rs4

```

### Corruptos 25%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos5$loan_status <- factor(corruptos5$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs5 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos5,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs5$best_model$minsplit,
    minbucket = rs5$best_model$minbucket,
    maxdepth  = rs5$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs5 <- predict(best_tree_rs5, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs5,
                levels = lvl,
                quiet = TRUE)

auc_test_rs5 <- auc(roc_test)
auc_test_rs5

```

### Corruptos 30%:

```{r}

lvl <- c("Rechazado", "Aprobado")
corruptos6$loan_status <- factor(corruptos6$loan_status, levels = lvl)
valid_data$loan_status <- factor(valid_data$loan_status, levels = lvl)
test_data$loan_status   <- factor(test_data$loan_status, levels = lvl)

# entrenar con los hiperparámetros encontrados
best_tree_rs6 <- rpart(
  loan_status ~ ., 
  data = rbind(corruptos6,valid_data), 
  method = "class",
  control = rpart.control(
    minsplit  = rs6$best_model$minsplit,
    minbucket = rs6$best_model$minbucket,
    maxdepth  = rs6$best_model$maxdepth,
    cp = 0,
    xval = 0
  )
)

# predecir probabilidades en test
probas_test_rs6 <- predict(best_tree_rs6, newdata = test_data, type = "prob")[, "Aprobado"]

# calcular curva ROC y AUC
roc_test <- roc(response = test_data$loan_status,
                predictor = probas_test_rs6,
                levels = lvl,
                quiet = TRUE)

auc_test_rs6 <- auc(roc_test)
auc_test_rs6

```

### Resultados resumidos:

```{r}
# Crear tabla resumen
auc_results <- data.frame(
  Corrupcion = c("5%", "10%", "15%", "20%", "25%", "30%"),
  AUC_Test = c(auc_test_rs1,
               auc_test_rs2,
               auc_test_rs3,
               auc_test_rs4,
               auc_test_rs5,
               auc_test_rs6)
)

print(auc_results)
```

**Gráfico de los diferentes AUC en testing para casa nivel de
corrupción:**

```{r}

auc_results <- data.frame(
  Corrupcion = c(5, 10, 15, 20, 25, 30),
  AUC_Test   = c(auc_test_rs1,
                 auc_test_rs2,
                 auc_test_rs3,
                 auc_test_rs4,
                 auc_test_rs5,
                 auc_test_rs6)
)

ggplot(auc_results, aes(x = Corrupcion, y = AUC_Test)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(size = 3, color = "darkred") +
  labs(title = "Evolución del AUC en Test según % de etiquetas corruptas",
       x = "% de corrupción en etiquetas",
       y = "AUC en Test") +
  theme_minimal(base_size = 13)

```

```{r}
roc_list <- list(
  "5%"  = roc(response = test_data$loan_status, predictor = probas_test_rs1, levels = lvl, quiet = TRUE),
  "10%" = roc(response = test_data$loan_status, predictor = probas_test_rs2, levels = lvl, quiet = TRUE),
  "15%" = roc(response = test_data$loan_status, predictor = probas_test_rs3, levels = lvl, quiet = TRUE),
  "20%" = roc(response = test_data$loan_status, predictor = probas_test_rs4, levels = lvl, quiet = TRUE),
  "25%" = roc(response = test_data$loan_status, predictor = probas_test_rs5, levels = lvl, quiet = TRUE),
  "30%" = roc(response = test_data$loan_status, predictor = probas_test_rs6, levels = lvl, quiet = TRUE)
)
```

**Gráfico de las diferentes curvas de AUC ROC en testing**

```{r}

cols <- c("5%"="blue","10%"="red","15%"="green","20%"="purple","25%"="orange","30%"="brown")

plot(roc_list[["5%"]], col=cols["5%"], lwd=2, main="Curvas ROC - distintos niveles de corrupción")
for (p in names(roc_list)[-1]) {
  plot(roc_list[[p]], col=cols[p], lwd=2, add=TRUE)
}
abline(0, 1, lty=2, col="gray")
legend("bottomright", legend=names(roc_list), col=cols, lwd=2, bty="n")
```

A medida que vamos sumando más corrupción en las etiquetas, el
rendimiento del modelo empieza a caer. Se puede ver que esa caída no es
lineal. Entre 5% y 15% de corrupción prácticamente no se nota el
impacto, el AUC se mantiene en un rango muy alto (alrededor de
0.965–0.963). Pero cuando pasamos el 20%, se empieza a ver una caída más
fuerte y consistente, que se acentúa en 25% y 30%. Esto nos muestra que
el árbol de decisión aguanta bien un nivel moderado de ruido, pero hay
un punto crítico (20–25%) a partir del cual la performance ya empeora
más rápido. Básicamente, con poco ruido la señal real todavía domina,
pero cuando las etiquetas malas son demasiadas, el modelo ya no logra
separar tan bien.

Si miramos los hiperparámetros que salen del random search, también
aparecen patrones. La profundidad máxima (maxdepth) varía bastante entre
escenarios y no sigue una tendencia clara, lo que indica que no es tan
sensible al ruido. En cambio, minsplit y minbucket sí muestran un
aumento claro cuando sube la corrupción: por ejemplo, de un minsplit de
48 en 5% pasamos a 497 en 30%. Esto significa que el árbol necesita
nodos más grandes para no sobreajustar al ruido, es decir, se queda con
mas información y genera modelos menos detallados para evitar aprender
reglas equivocadas. En paralelo, el AUC de validación sigue la misma
linea que el de test, muy estable hasta 15% y después empieza a caer.

Podemos ver que el modelo soporta bastante bien un nivel bajo de
etiquetas corruptas, pero cuando superamos el 20% la baja en performance
se nota mucho más. Lo que vemos es un comportamiento por umbrales, hay
una zona de tolerancia y luego un punto a partir del cual la performance
cae. Además, los hiperparámetros se ajustan de manera lógica frente al
ruido, aumentan minsplit y minbucket, lo que da árboles más simples y
generales.

##Ejercicio 7

falta entero